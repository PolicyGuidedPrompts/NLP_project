We added two meanings to special action:
stop adding in context examples and generate output.
In practice:
special action need to mean stop adding examples only. generating happens regardless.

Policy started choosing same action for all examples.

gpt2 shit

chooses same action all the time

The policy seems to not really care about if question itself but rather put more attention to how the question answer is
formatted

put two different logic of softmax temperature in order to encourage exploration

see if can prove correlation between prompt length and better predictions

prompt shorter than 300 results in YES/NO instead of True/False

Fact -> Question > Answer resulted in "True Facts" instead of "True", switch the order Question -> Fact -> Answer helped

longer prompts takes longer to get stuck on a specific action

Prompt too long (more than 500 tokens cause poor predictions, instead of True/False continue sentence not even starting with True/False)

When using retriever and we allow our policy to choose form the top_k questions most resemble to it, the obvious thing happens,
Meaning given question A, the prompt that is autogenerated for it is A,A,A,A. Which make sense.

Dataset with high context like squad, model chooses not to add context

Dataset like Trivia, model chooses to add some context (if some questions can add value)

Dataset like Math answering expect to work best in context

Small model can not take too much context, hence limited with its learning abilities

Some mistakes can be easily explained by the fact that parsing the data is not optimal due to time limitations

Tried custom dataset open tdb, dataset is not good enough, missing context

Tried math deepmind dataset not good enough because there is not solution, only final answer, no gradients to learn from

Too easy dataset like squad/strategy-qa which the question itself contain the answer (reading comprehension) results in choosing action 0